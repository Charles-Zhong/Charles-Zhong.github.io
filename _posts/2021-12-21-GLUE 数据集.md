---
layout: post
section-type: post
title: GLUE 数据集
category: tech
tags: [ 'tutorial' ]
---

# GLUE 数据集
以下网站可以查看数据集详情。
https://huggingface.co/datasets/viewer/?dataset=glue
## 数据集摘要
GLUE，通用语言理解评估基准(https://gluebenchmark.com/)是一个用于训练、评估和分析自然语言理解系统的资源集合。
## 数据集大小整理
|Task|Train|Validation|Test|
|--|--|--|--|
|Cola|8551|1043|1063|
|SST-2|67349|872|1821|
|MRPC|3668|408|1725|
|QQP|363846|40430|390965|
|STS-B|5749|1500|1379|
|MNLI|392702|9815 / 9832|9796 / 9847|
|QNLI|104743|5463|5463|
|RTE|2490|277|3000|
|WNLI|635|71|146|
|AX|/|/|1104|
## 支持的任务和排行榜
GLUE基准的排行榜可以在这个[地址](https://gluebenchmark.com/)找到。它包括以下任务:
### (1) ax
一个手动管理的评估数据集，用于对广泛语言现象的系统性能进行细粒度分析。该数据集通过自然语言推理（NLI）问题评估句子理解。使用一个在MulitNLI上训练过的模型为这个数据集生成预测。
```
{
  "premise": "string",
  "hypothesis": "string",
  "label": [
    "entailment",
    "neutral",
    "contradiction"
  ],
  "idx": "int32"
}
```
### (2) cola
语言可接受性语料库包括从语言理论书籍和期刊文章中摘录的英语可接受性判断。每个例子都是一个单词序列，注明它是否是一个符合语法的英语句子。
```
{
  "sentence": "string",
  "label": [
    "unacceptable",
    "acceptable"
  ],
  "idx": "int32"
}
```
### (3) mnli
多体裁自然语言推理语料库（Multi-general Language推断）是一个具有文本蕴涵注释的句子对的众包集合。给定一个前提句和一个假设句，任务是预测前提是否包含假设（蕴涵），是否与假设（矛盾），或者两者都不是（中立）。前提句来自十个不同的来源，包括转录的演讲、小说和政府报告。基准的作者使用标准的测试集，他们从RTE作者那里获得私有标签，并对匹配（域内）和不匹配（跨域）部分进行评估。他们还使用并推荐SNLI语料库作为550k个辅助训练数据的例子。
```
{
  "premise": "string",
  "hypothesis": "string",
  "label": [
    "entailment",
    "neutral",
    "contradiction"
  ],
  "idx": "int32"
}
```
### (3-1) mnli_matched
从MNLI分离出来的匹配验证和测试。有关更多信息，请参阅mnli构建配置。
### (3-2) mnli_mismatched
从MNLI分离出来的不匹配验证和测试。有关更多信息，请参阅mnli构建配置。
### (4) mrpc
微软研究院的释义语料库（Dolan&Brockett，2005）是一个自动从在线新闻源中提取的句子对语料库，带有人工注释，用于判断句子对中的句子在语义上是否等价。
```
{
  "sentence1": "string",
  "sentence2": "string",
  "label": [
    "not_equivalent",
    "equivalent"
  ],
  "idx": "int32"
}
```
### (5) qnli
斯坦福问答数据集是由问题-段落对组成的问答数据集，其中段落中的一个句子（摘自维基百科）包含相应问题的答案（由注释者编写）。基准的作者通过在相应上下文中的每个问题和每个句子之间形成一对，并过滤出问题和上下文句子之间词汇重叠度较低的对，从而将任务转换为句子对分类。任务是确定上下文句子是否包含问题的答案。这个原始任务的修改版本取消了模型选择准确答案的要求，但也取消了简化的假设，即答案总是存在于输入中，词汇重叠是一个可靠的提示。
```
{
  "question": "string",
  "sentence": "string",
  "label": [
    "entailment",
    "not_entailment"
  ],
  "idx": "int32"
}
```
### (6) qqp
Quora questions Pairs2数据集是来自社区问答网站Quora的问题对的集合。任务是确定一对问题在语义上是否等价。
```
{
  "question1": "string",
  "question2": "string",
  "label": [
    "not_duplicate",
    "duplicate"
  ],
  "idx": "int32"
}
```
### (7) rte
识别文本蕴含(RTE)数据集来自于一系列年度文本蕴含挑战。基准测试的作者结合了RTE1 (Dagan et al.， 2006)、RTE2 (Bar Haim et al.， 2006)、RTE3 (Giampiccolo et al.， 2007)和RTE5 (Bentivogli et al.， 2009)的数据。示例是基于新闻和维基百科文本构建的。基准测试的作者将所有数据集转换为两类分割，对于三类数据集，为了一致性，他们将中立和矛盾分解为不包含。
```
{
  "sentence1": "string",
  "sentence2": "string",
  "label": [
    "entailment",
    "not_entailment"
  ],
  "idx": "int32"
}
```
### (8) sst2
斯坦福情感树库由电影评论中的句子和人类对其情感的注释组成。任务是预测给定句子的情绪。它使用双向（正/负）类别拆分，只有句子级标签。
```
{
  "sentence": "string",
  "label": [
    "negative",
    "positive"
  ],
  "idx": "int32"
}
```
### (9) stsb
语义-文本相似性基准（Cer等人，2017）是从新闻标题、视频和图像标题以及自然语言推理数据中提取的句子对的集合。每一对都是人类注释，相似性分数从0到5。
```
{
  "sentence1": "string",
  "sentence2": "string",
  "label": "float32",
  "idx": "int32"
}
```
### (10) wnli
Winograd Schema Challenge（Levesque等人，2011）是一项阅读理解任务，系统必须阅读带有代词的句子，并从选择列表中选择该代词的指称。这些例子是手工构建的，以挫败简单的统计方法：每个例子都取决于句子中单个单词或短语提供的上下文信息。为了将这个问题转化为句子对的分类，基准的作者通过用每个可能的参照物替换歧义代词来构造句子对。这项任务是预测代词被替换的句子是否与原句子有关。他们使用了一个小的评估集，其中包含了来自小说的新例子，这些小说是由原始语料库的作者私下分享的。虽然包含的训练集在两个类之间是平衡的，但测试集在两个类之间是不平衡的（65%不包含）。另外，由于数据的怪异，开发集是对抗的：假设有时在训练和开发示例之间共享，因此如果一个模型记住了训练示例，他们将在相应的开发集样例上预测错误的标记。与QNLI一样，每个示例都是单独评估的，因此模型在该任务上的得分与其在未转换的原始任务上的得分之间没有系统的对应关系。基准调用的作者转换了数据集WNLI（Winograd NLI）。
```
{
  "sentence1": "string",
  "sentence2": "string",
  "label": [
    "not_entailment",
    "entailment"
  ],
  "idx": "int32"
}
```